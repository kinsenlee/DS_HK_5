{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.load_remote_theme = false\n",
       "var theme_url = \"https://drostehk.github.io/ipynb-theme/\";\n",
       "var asset_url = 'https://raw.githubusercontent.com/tijptjik/DS_assets/master/';\n",
       "\n",
       "window.load_local_theme = function(){\n",
       "    var hostname = document.location.hostname\n",
       "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
       "}\n",
       "\n",
       "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_url + 'custom.js'\n",
       "\n",
       "$.getScript(url)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.load_remote_theme = false\n",
    "var theme_url = \"https://drostehk.github.io/ipynb-theme/\";\n",
    "var asset_url = 'https://raw.githubusercontent.com/tijptjik/DS_assets/master/';\n",
    "\n",
    "window.load_local_theme = function(){\n",
    "    var hostname = document.location.hostname\n",
    "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
    "}\n",
    "\n",
    "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_url + 'custom.js'\n",
    "\n",
    "$.getScript(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> “Show me your code and conceal your data structures, and I shall continue to be mystified. Show me your data structures, and I won’t usually need your code; it’ll be obvious.” \n",
    "\n",
    "<footer>**Eric Raymond**, in The Cathedral and the Bazaar, 1997</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgs.xkcd.com/comics/exploits_of_a_mom.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![break](assets/agenda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Structures : Flat or Go Deep?\n",
    "1. Getting freaky with `pandas`\n",
    "1. Abstracting the query with `blaze`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To  explore data modeling techniques, we have to start with a more or less systematic view of NoSQL data models that preferably reveals trends and interconnections. The following figure depicts imaginary “evolution” of the major NoSQL system families, namely, Key-Value stores, BigTable-style databases, Document databases, Full Text Search Engines, and Graph databases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://highlyscalable.files.wordpress.com/2012/02/overview2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Serialisation Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When saving or communicating some kind of information, we often use serialization. Serialization takes a Python object and converts it into a string of bytes and vice versa. For example, if you have an object representing information about a user and need to send it over the network, it has to be serialized into a set of bytes that can be pushed over a socket. Then, at the other end, the receiver has to unserialize the object, converting it back into something that Python (or another language) can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic problem is this: CSVs have inherent schemas. In fact, most of the CSVs that I work with are dumps from a database. While the database can maintain schema information alongside the data, the scheme is lost when serializing to disk. Worse, if the dump is denormalized (a join of two tables), then the relationships are also lost, making it harder to extract entities. Although a header row can give us the names of the fields in the file, it won't give us the type, and there is nothing structural about the serialization format (like there is with JSON) that we can infer the type from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But we've got pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     permalink      company  numEmps category        city state fundedDate  \\\n",
      "0     lifelock     LifeLock      NaN      web       Tempe    AZ   1-May-07   \n",
      "1     lifelock     LifeLock      NaN      web       Tempe    AZ   1-Oct-06   \n",
      "2     lifelock     LifeLock      NaN      web       Tempe    AZ   1-Jan-08   \n",
      "3  mycityfaces  MyCityFaces        7      web  Scottsdale    AZ   1-Jan-08   \n",
      "4     flypaper     Flypaper      NaN      web     Phoenix    AZ   1-Feb-08   \n",
      "\n",
      "   raisedAmt raisedCurrency round  \n",
      "0    6850000            USD     b  \n",
      "1    6000000            USD     a  \n",
      "2   25000000            USD     c  \n",
      "3      50000            USD  seed  \n",
      "4    3000000            USD     a  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(DATA_DIR + 'funding.csv')\n",
    "print data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utilities for analysis that `Pandas` gives you, especially `DataFrames`, are extremely useful, and there is obviously a 1:1 relationship between DataFrames and CSV files. We routinely use Pandas for data analyses, quick insights, and even data wrangling of smaller files.\n",
    "\n",
    "The problem is that Pandas is not meant for production-level ingestion or wrangling systems. It is meant for data analysis that can happen completely in memory. As such, when you run this line of code, the entirety of the CSV file is loaded into memory. Likewise, Numpy arrays are also immutable data types that are completely loaded into memory. You've just lost your memory efficiency, especially for larger data sets.\n",
    "\n",
    "And while this may be fine on your laptop, keep in mind that if you're writing data pipeline code, it will probably be run more routinely on a virtual cloud server such as Rackspace, AWS, or Google App Engine. Since you have a budget, it will also probably be running on small or micro servers that might have 1 GB of memory, if you're lucky. You don't want to blow it away!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built-in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifeLock ( employees) raised 6850000 on 1-May-07\n",
      "LifeLock ( employees) raised 6000000 on 1-Oct-06\n",
      "LifeLock ( employees) raised 25000000 on 1-Jan-08\n",
      "MyCityFaces (7 employees) raised 50000 on 1-Jan-08\n",
      "Flypaper ( employees) raised 3000000 on 1-Feb-08\n",
      "Infusionsoft (105 employees) raised 9000000 on 1-Oct-07\n",
      "gAuto (4 employees) raised 250000 on 1-Jan-08\n",
      "ChosenList.com (5 employees) raised 140000 on 1-Oct-06\n",
      "ChosenList.com (5 employees) raised 233750 on 25-Jan-08\n",
      "Digg (60 employees) raised 8500000 on 1-Dec-06\n",
      "Digg (60 employees) raised 2800000 on 1-Oct-05\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def read_funding_data(path):\n",
    "    with open(path, 'rU') as data:\n",
    "        reader = csv.DictReader(data)\n",
    "        for row in reader:\n",
    "            yield row\n",
    "\n",
    "\n",
    "for idx, row in enumerate(read_funding_data(DATA_DIR + 'funding.csv')):\n",
    "    if idx > 10: break\n",
    "    print \"%(company)s (%(numEmps)s employees) raised %(raisedAmt)s on %(fundedDate)s\" % row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of key points with the code above:\n",
    "\n",
    "* Always wrap the `CSV` reader in a function that returns a generator (via the yield statement).\n",
    "*  Open the file in universal newline mode with 'rU' for backwards compatibility.\n",
    "* Use context managers with [callable] as [name] to ensure that the handle to the file is closed automatically.\n",
    "* Use the `csv.DictReader` class only when headers are present, otherwise just use `csv.reader`. (You can pass a list of fieldnames, but you'll see its better just to use a namedtuple as we discuss below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object read_funding_data at 0x7fd2466c1f50>\n"
     ]
    }
   ],
   "source": [
    "data = read_funding_data(DATA_DIR + 'funding.csv')\n",
    "print repr(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that the function \"returns\" a generator, thanks to the yield statement in the function definition. This means, among other things, that the data is evaluated lazily. The file is not opened, read, or parsed until you need it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML is a widely used format for data exchange, because it gives good opportunities to keep the structure in the data and the way files are built on and allows developers to write parts of the documentation in with the data without interfering with the reading of them. This is pretty easy in Python as well. You will need the MiniDom library. It is also preinstalled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x =\"\"\"<data>\n",
    "    <items>\n",
    "        <item name=\"item1\"></item>\n",
    "        <item name=\"item2\"></item>\n",
    "        <item name=\"item3\"></item>\n",
    "        <item name=\"item4\"></item>\n",
    "    </items>\n",
    "</data>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML is a tree-like structure, while a Pandas DataFrame is a 2D table-like structure. So there is no automatic way to convert between the two. You have to understand the XML structure and know how you want to map its data onto a 2D table. Thus, every XML-to-DataFrame problem is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<data>\\n    <items>\\n        <item name=\"item1\"></item>\\n        <item name=\"item2\"></item>\\n        <item name=\"item3\"></item>\\n        <item name=\"item4\"></item>\\n    </items>\\n</data>\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<DOM Element: item at 0x7fd235dcecf8>, <DOM Element: item at 0x7fd235dce560>, <DOM Element: item at 0x7fd235dceea8>, <DOM Element: item at 0x7fd235d7f1b8>]\n"
     ]
    }
   ],
   "source": [
    "from xml.dom import minidom\n",
    "\n",
    "xmldoc = minidom.parseString(x)\n",
    "itemlist = xmldoc.getElementsByTagName('item')\n",
    "\n",
    "print itemlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(itemlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item1\n"
     ]
    }
   ],
   "source": [
    "print(itemlist[0].attributes['name'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item1\n",
      "item2\n",
      "item3\n",
      "item4\n"
     ]
    }
   ],
   "source": [
    "for s in itemlist:\n",
    "    print(s.attributes['name'].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple analysis, however, we just want a rectangular data-frame with columns and rows and we need to flatten all that structure. The following code does a very simple job of converting an XML file into a Pandas data-frame. It recursively parses every branch in the file creating new columns and storing their value when information is found. It stores not just raw text as variables in the new dataset, but also all of the attributes stored in tags as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def xml2df(xml_doc):\n",
    "    f = open(xml_doc, 'r')\n",
    "    soup = BeautifulSoup(f)\n",
    "    name_list=[]\n",
    "    text_list=[]\n",
    "    attr_list=[]\n",
    "\n",
    "    def recurs(soup):\n",
    "        try:\n",
    "            for j in soup.contents:\n",
    "                try:\n",
    "                    #print j.name\n",
    "                    if j.name!=None:\n",
    "                        name_list.append(j.name)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    #print j.text\n",
    "                    if j.name!=None:\n",
    "                        #print j.string\n",
    "                        text_list.append(j.string)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    #print j.attrs\n",
    "                    if j.name!=None:\n",
    "                        attr_list.append(j.attrs)\n",
    "                except:\n",
    "                    pass\n",
    "                recurs(j)\n",
    "        except:\n",
    "            pass\n",
    "    recurs(soup)\n",
    "    attr_names_list = [q.keys() for q in attr_list]\n",
    "    attr_values_list = [q.values() for q in attr_list]\n",
    "    columns = hstack((hstack(name_list),\n",
    "                      hstack(attr_names_list)) )\n",
    "    data = hstack((hstack(text_list),\n",
    "                   hstack(attr_values_list)) )\n",
    "    df = pd.DataFrame(data=matrix(data.T), columns=columns )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON is a simple file format that is very easy for any programming language to read. Its simplicity means that it is generally easier for computers to process than others, such as XML. Working with JSON in Python is almost the same such as working with a Python dictionary. You will need the JSON library, but it is preinstalled to every Python 2.6 and after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"glossary\": {\n",
    "        \"title\": \"example glossary\",\n",
    "\t\t\"GlossDiv\": {\n",
    "            \"title\": \"S\",\n",
    "\t\t\t\"GlossList\": {\n",
    "                \"GlossEntry\": {\n",
    "                    \"ID\": \"SGML\",\n",
    "\t\t\t\t\t\"SortAs\": \"SGML\",\n",
    "\t\t\t\t\t\"GlossTerm\": \"Standard Generalized Markup Language\",\n",
    "\t\t\t\t\t\"Acronym\": \"SGML\",\n",
    "\t\t\t\t\t\"Abbrev\": \"ISO 8879:1986\",\n",
    "\t\t\t\t\t\"GlossDef\": {\n",
    "                        \"para\": \"A meta-markup language, used to create markup languages such as DocBook.\",\n",
    "\t\t\t\t\t\t\"GlossSeeAlso\": [\"GML\", \"XML\"]\n",
    "                    },\n",
    "\t\t\t\t\t\"GlossSee\": \"markup\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the same as the `XML`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```xml\n",
    "<!DOCTYPE glossary PUBLIC \"-//OASIS//DTD DocBook V3.1//EN\">\n",
    " <glossary><title>example glossary</title>\n",
    "  <GlossDiv><title>S</title>\n",
    "   <GlossList>\n",
    "    <GlossEntry ID=\"SGML\" SortAs=\"SGML\">\n",
    "     <GlossTerm>Standard Generalized Markup Language</GlossTerm>\n",
    "     <Acronym>SGML</Acronym>\n",
    "     <Abbrev>ISO 8879:1986</Abbrev>\n",
    "     <GlossDef>\n",
    "      <para>A meta-markup language, used to create markup\n",
    "languages such as DocBook.</para>\n",
    "      <GlossSeeAlso OtherTerm=\"GML\">\n",
    "      <GlossSeeAlso OtherTerm=\"XML\">\n",
    "     </GlossDef>\n",
    "     <GlossSee OtherTerm=\"markup\">\n",
    "    </GlossEntry>\n",
    "   </GlossList>\n",
    "  </GlossDiv>\n",
    " </glossary>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with JSON later on in today's class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAML is a recursive acronym that stands for “YAML Ain’t Markup Language”. It is a serialization format, but it is also (easily) human readable, meaning that it can be used as a configuration language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Jesse'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "yaml.load(\"\"\"\n",
    "# YAML\n",
    "name: Jesse\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple example. Line 1 of the YAML file, or document, is a simple comment. Note that there is a space character right before that # sign. The next line is a simple key value pair which, after being parsed, gets returned to us in a Python dictionary. Simple as pie!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple name-value pair is easy to do. Here is a document with some additional structures and details to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': {'attributes': ['attr1', 'attr2', 'attr3'],\n",
       "  'methods': ['getter', 'setter']}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml.load(\"\"\"\n",
    "# YAML\n",
    "object:\n",
    "    attributes:\n",
    "        - attr1\n",
    "        - attr2\n",
    "        - attr3\n",
    "    methods: [ getter, setter ]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have defined a top-level entity named \"object\". This object has two block mappings related to it, ''attributes'' and ''methods''. The ''attributes'' mapping uses the more verbose YAML syntax for a list, in this case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "attributes:\n",
    "    - attr1\n",
    "    - attr2\n",
    "    - attr3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an extension treatmeent of YAML in Python see this [blogpost](http://jessenoller.com/blog/2009/04/13/yaml-aint-markup-language-completely-different)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " API response: {\"id\":16449298,\"url\":\"https://api.github.com/authorizations/16449298\",\"app\":{\"name\":\"Mining the Social Web, 2nd Ed. (API)\",\"url\":\"https://developer.github.com/v3/oauth_authorizations/\",\"client_id\":\"00000000000000000000\"},\"token\":\"082421eee874278af653cf8118e03bfc80476cb3\",\"note\":\"Mining the Social Web, 2nd Ed.\",\"note_url\":null,\"created_at\":\"2015-03-25T08:22:22Z\",\"updated_at\":\"2015-03-25T08:22:22Z\",\"scopes\":[\"repo\"]}\n",
      "\n",
      "Your OAuth token is 082421eee874278af653cf8118e03bfc80476cb3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from getpass import getpass\n",
    "import json\n",
    "\n",
    "username = 'tijptjik' # Your GitHub username\n",
    "password = PASSWORD # Your GitHub password\n",
    "\n",
    "# Note that credentials will be transmitted over a secure SSL connection\n",
    "url = 'https://api.github.com/authorizations'\n",
    "note = 'Data Wrangling with GitHub '\n",
    "post_data = {'scopes':['repo'],'note': note }\n",
    "\n",
    "response = requests.post(\n",
    "    url,\n",
    "    auth = (username, password),\n",
    "    data = json.dumps(post_data),\n",
    "    )   \n",
    "\n",
    "print \"API response:\", response.text\n",
    "print\n",
    "print \"Your OAuth token is\", response.json()['token']\n",
    "\n",
    "# Go to https://github.com/settings/applications to revoke this token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making direct HTTP requests to GitHub's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"following_url\": \"https://api.github.com/users/hammer/following{/other_user}\", \n",
      " \"events_url\": \"https://api.github.com/users/hammer/events{/privacy}\", \n",
      " \"organizations_url\": \"https://api.github.com/users/hammer/orgs\", \n",
      " \"url\": \"https://api.github.com/users/hammer\", \n",
      " \"gists_url\": \"https://api.github.com/users/hammer/gists{/gist_id}\", \n",
      " \"html_url\": \"https://github.com/hammer\", \n",
      " \"subscriptions_url\": \"https://api.github.com/users/hammer/subscriptions\", \n",
      " \"avatar_url\": \"https://avatars.githubusercontent.com/u/15233?v=3\", \n",
      " \"repos_url\": \"https://api.github.com/users/hammer/repos\", \n",
      " \"received_events_url\": \"https://api.github.com/users/hammer/received_events\", \n",
      " \"gravatar_id\": \"\", \n",
      " \"starred_url\": \"https://api.github.com/users/hammer/starred{/owner}{/repo}\", \n",
      " \"site_admin\": false, \n",
      " \"login\": \"hammer\", \n",
      " \"type\": \"User\", \n",
      " \"id\": 15233, \n",
      " \"followers_url\": \"https://api.github.com/users/hammer/followers\"\n",
      "}\n",
      "\n",
      "vary => Accept, Accept-Encoding\n",
      "x-served-by => 139317cebd6caf9cd03889139437f00b\n",
      "x-xss-protection => 1; mode=block\n",
      "x-content-type-options => nosniff\n",
      "etag => W/\"6958416666749bcfb38f94c13d260f4f\"\n",
      "access-control-allow-credentials => true\n",
      "status => 200 OK\n",
      "x-ratelimit-remaining => 57\n",
      "x-github-media-type => github.v3\n",
      "access-control-expose-headers => ETag, Link, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval\n",
      "transfer-encoding => chunked\n",
      "x-github-request-id => B6EF6D60:656F:194D777:551270F1\n",
      "cache-control => public, max-age=60, s-maxage=60\n",
      "link => <https://api.github.com/repositories/843222/stargazers?page=2>; rel=\"next\", <https://api.github.com/repositories/843222/stargazers?page=180>; rel=\"last\"\n",
      "date => Wed, 25 Mar 2015 08:25:22 GMT\n",
      "access-control-allow-origin => *\n",
      "content-security-policy => default-src 'none'\n",
      "content-encoding => gzip\n",
      "strict-transport-security => max-age=31536000; includeSubdomains; preload\n",
      "server => GitHub.com\n",
      "x-ratelimit-limit => 60\n",
      "x-frame-options => deny\n",
      "content-type => application/json; charset=utf-8\n",
      "x-ratelimit-reset => 1427275331\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# An unauthenticated request that doesn't contain an ?access_token=xxx query string\n",
    "url = \"https://api.github.com/repos/scikit-learn/scikit-learn/stargazers\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Display one stargazer\n",
    "\n",
    "print json.dumps(response.json()[0], indent=1)\n",
    "print\n",
    "\n",
    "# Display headers\n",
    "for (k,v) in response.headers.items():\n",
    "    print k, \"=>\", v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygithub\n",
      "  Downloading PyGithub-1.25.2.tar.gz (2.6MB)\n",
      "\u001b[K    100% |################################| 2.6MB 160kB/s \n",
      "\u001b[?25hInstalling collected packages: pygithub\n",
      "  Running setup.py install for pygithub\n",
      "Successfully installed pygithub-1.25.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pygithub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyGithub to query for stargazers of a particular repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stargazers 4\n"
     ]
    }
   ],
   "source": [
    "from github import Github\n",
    "\n",
    "# XXX: Specify your own access token here\n",
    "\n",
    "ACCESS_TOKEN = '082421eee874278af653cf8118e03bfc80476cb3'\n",
    "\n",
    "# Specify a username and repository of interest for that user.\n",
    "\n",
    "USER = 'tijptjik'\n",
    "REPO = 'SyntheticSynthesis'\n",
    "\n",
    "client = Github(ACCESS_TOKEN, per_page=100)\n",
    "user = client.get_user(USER)\n",
    "repo = user.get_repo(REPO)\n",
    "\n",
    "# Get a list of people who have bookmarked the repo.\n",
    "# Since you'll get a lazy iterator back, you have to traverse\n",
    "# it if you want to get the total number of stargazers.\n",
    "\n",
    "stargazers = [ s for s in repo.get_stargazers() ]\n",
    "print \"Number of stargazers\", len(stargazers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<github.NamedUser.NamedUser at 0x7f3e1d761750>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stargazers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a trivial property graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 2\n",
      "Number of edges: 1\n",
      "Average in degree:   0.5000\n",
      "Average out degree:   0.5000\n",
      "\n",
      "Nodes: ['Y', 'X']\n",
      "Edges: [('X', 'Y')]\n",
      "\n",
      "X props: {}\n",
      "Y props: {}\n",
      "X=>Y props: {}\n",
      "\n",
      "X props: {'prop1': 'value1'}\n",
      "\n",
      "X=>Y props: {'label': 'label1'}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "\n",
    "g = nx.DiGraph()\n",
    "\n",
    "# Add an edge to the directed graph from X to Y\n",
    "\n",
    "g.add_edge('X', 'Y')\n",
    "\n",
    "# Print some statistics about the graph\n",
    "\n",
    "print nx.info(g)\n",
    "print\n",
    "\n",
    "# Get the nodes and edges from the graph\n",
    "\n",
    "print \"Nodes:\", g.nodes()\n",
    "print \"Edges:\", g.edges()\n",
    "print\n",
    "\n",
    "# Get node properties\n",
    "\n",
    "print \"X props:\", g.node['X']\n",
    "print \"Y props:\", g.node['Y']\n",
    "\n",
    "# Get edge properties\n",
    "\n",
    "print \"X=>Y props:\", g['X']['Y']\n",
    "print\n",
    "\n",
    "# Update a node property\n",
    "\n",
    "g.node['X'].update({'prop1' : 'value1'})\n",
    "print \"X props:\", g.node['X']\n",
    "print\n",
    "\n",
    "# Update an edge property\n",
    "\n",
    "g['X']['Y'].update({'label' : 'label1'})\n",
    "print \"X=>Y props:\", g['X']['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expand the initial graph with (interest) edges pointing each\n",
    "# direction for additional people interested. Take care to ensure\n",
    "# that user and repo nodes do not collide by appending their type.\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "g = nx.DiGraph()\n",
    "g.add_node(repo.name + '(repo)', type='repo', lang=repo.language, owner=user.login)\n",
    "\n",
    "for sg in stargazers:\n",
    "    g.add_node(sg.login + '(user)', type='user')\n",
    "    g.add_edge(sg.login + '(user)', repo.name + '(repo)', type='gazes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 5\n",
      "Number of edges: 4\n",
      "Average in degree:   0.8000\n",
      "Average out degree:   0.8000\n",
      "\n",
      "{'lang': None, 'owner': u'tijptjik', 'type': 'repo'}\n",
      "{'type': 'user'}\n",
      "\n",
      "{'type': 'gazes'}\n",
      "\n",
      "{u'SyntheticSynthesis(repo)': {'type': 'gazes'}}\n",
      "{}\n",
      "\n",
      "[]\n",
      "[('tijptjik(user)', u'SyntheticSynthesis(repo)')]\n",
      "\n",
      "[(u'1000camels(user)', 'SyntheticSynthesis(repo)'), (u'nodu(user)', 'SyntheticSynthesis(repo)'), (u'tijptjik(user)', 'SyntheticSynthesis(repo)'), (u'szs(user)', 'SyntheticSynthesis(repo)')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Poke around in the current graph to get a better feel for how NetworkX works\n",
    "\n",
    "print nx.info(g)\n",
    "print\n",
    "print g.node['SyntheticSynthesis(repo)']\n",
    "print g.node['tijptjik(user)']\n",
    "print\n",
    "print g['tijptjik(user)']['SyntheticSynthesis(repo)']\n",
    "print\n",
    "print g['tijptjik(user)']\n",
    "print g['SyntheticSynthesis(repo)']\n",
    "print\n",
    "print g.in_edges(['tijptjik(user)'])\n",
    "print g.out_edges(['tijptjik(user)'])\n",
    "print\n",
    "print g.in_edges(['SyntheticSynthesis(repo)'])\n",
    "print g.out_edges(['SyntheticSynthesis(repo)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a No-SQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head to the [MongoDB download page](http://www.mongodb.org/downloads)  and get the latest stable version) for your OS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip `mongodb-X.tgz` and rename folder `mongodb-X` to simply `mongodb`. I normally move this folder to my personal project folder so feel free to move it in wherever you like. Your mongodb folder should have the following structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "GNU-AGPL-3.0        README              THIRD-PARTY-NOTICES bin\n",
    "mongodb/bin:\n",
    "bsondump     mongod       mongoexport  mongoimport  mongoperf    mongos       mongostat\n",
    "mongo        mongodump    mongofiles   mongooplog   mongorestore mongosniff   mongotop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### startserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd mongodb\n",
    "./bin/mongod --dbpath . --nojournal &\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open another terminal windows and type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "~/YOURPATH/mongodb/bin/mongo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "MongoDB shell version: 2.4.9\n",
    "connecting to: test\n",
    "Welcome to the MongoDB shell.\n",
    "For interactive help, type \"help\".\n",
    "For more comprehensive documentation, see\n",
    "\thttp://docs.mongodb.org/\n",
    "Questions? Try the support group\n",
    "\thttp://groups.google.com/group/mongodb-user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then you are good to go! Type exit to quit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyMongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMongo is a Python distribution containing tools for working with MongoDB, and is the recommended way to work with MongoDB from Python. You can install mongodb from either pip or easy_install.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pymongo 2.4.1 the Connection() method has been deprecated. Now we most use MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MongoClient('localhost', 27017)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "# Connection to Mongo DB\n",
    "try:\n",
    "    conn=pymongo.MongoClient()\n",
    "    print \"Connected successfully!!!\"\n",
    "except pymongo.errors.ConnectionFailure, e:\n",
    "   print \"Could not connect to MongoDB: %s\" % e \n",
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mongodb creates databases and collections automatically for you if they don't exist already. A single instance of MongoDB can support multiple independent databases. When working with PyMongo you access databases using attribute style access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient('localhost', 27017), u'tweets')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = conn.tweets\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your database name is such that using attribute style access won’t work (like db-name), you can use dictionary style access instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient('localhost', 27017), u'data-science')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = conn['data-science']\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to know what databases are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'local', u'db']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already created 2 new databases. Why didn't show up with the above command? Well, databases with no collections or with empty collections will not show up with database_names(). Same goes when we try to list empty collections in a database.\n",
    "\n",
    "We'll test it again once we have populate some collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection is a group of documents stored in MongoDB, and can be thought of as roughly the equivalent of a table in a relational database. Getting a collection in PyMongo works the same as getting a database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient('localhost', 27017), u'data-science'), u'tweets')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = db.tweets\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However one must be careful when trying to get existing collections. For example, if you have a collection db.user and you type db.usr this is clearly a mistake. Unlike an RDBMS, MongoDB won't protect you from this class of mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB stores structured data as JSON-like documents, using dynamic schemas (called BSON), rather than predefined schemas. An element of data is called a document, and documents are stored in collections. One collection may have any number of documents.\n",
    "\n",
    "Compared to relational databases, we could say collections are like tables, and documents are like records. But there is one big difference: every record in a table has the same fields (with, usually, differing values) in the same order, while each document in a collection can have completely different fields from the other documents.\n",
    "\n",
    "All you really need to know when you're using Python, however, is that documents are Python dictionaries that can have strings as keys and can contain various primitive types (int, float,unicode, datetime) as well as other documents (Python dicts) and arrays (Python lists).\n",
    "\n",
    "To insert some data into MongoDB, all we need to do is create a dict and call .insert() on the collection object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = {\"name\":\"Alberto\",\"surname\":\"Negron\",\"twitter\":\"@Altons\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to insert the above document into a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectId('5512834769038223b93eb3e5')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.insert(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'local', u'data-science', u'db']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'system.indexes', u'tweets']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations you have created your first document!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `database_names()` and `collection_names()` show database data-science and collection `tweets`.\n",
    "\n",
    "To recap, we have **databases** containing **collections**. A **collection** is made up of **documents**. Each document is made up of **fields**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): tweepy in /home/io/.tools/anaconda/lib/python2.7/site-packages\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests>=2.4.3 in /home/io/.tools/anaconda/lib/python2.7/site-packages (from tweepy)\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.7.3 in /home/io/.tools/anaconda/lib/python2.7/site-packages (from tweepy)\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests-oauthlib>=0.4.1 in /home/io/.tools/anaconda/lib/python2.7/site-packages (from tweepy)\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): oauthlib>=0.6.2 in /home/io/.tools/anaconda/lib/python2.7/site-packages (from requests-oauthlib>=0.4.1->tweepy)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the [authentication tutorial](http://docs.tweepy.org/en/v3.2.0/auth_tutorial.html#auth-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Get Twitter API Key](https://apps.twitter.com/app/new)\n",
    "\n",
    "Be sure to allow your app to also havewrite access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = '1tC8FNuMiVfFa4PvcuizVI2NF'\n",
    "consumer_secret = 'eDrVwrlkl3Qr6F8DaNOhuyOYlt7lSZeifDPo4w3nvi56TPaMPc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please authorize: https://api.twitter.com/oauth/authorize?oauth_token=l4qXsd0AI2COTK2BVcbqhELGx6l4nlnH\n"
     ]
    }
   ],
   "source": [
    "auth_url = auth.get_authorization_url()\n",
    "print 'Please authorize: ' + auth_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'69573308-Djl4EcKPcu2H8SwuvqAPjrW7ARhZEdvU7TVArUl4n',\n",
       " u'lcFUEQ3p3zbeQHiXuPnPRsRQm8bFN31q4iAHSyeZiU7EH')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PASTE_YOUR_PIN = '1536404'\n",
    "auth.get_access_token(PASTE_YOUR_PIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS_KEY = 69573308-Djl4EcKPcu2H8SwuvqAPjrW7ARhZEdvU7TVArUl4n\n"
     ]
    }
   ],
   "source": [
    "print \"ACCESS_KEY =\" , auth.access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ACCESS_SECRET = lcFUEQ3p3zbeQHiXuPnPRsRQm8bFN31q4iAHSyeZiU7EH\n"
     ]
    }
   ],
   "source": [
    "print 'ACCESS_SECRET =', auth.access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# access_token = '69573308-BJpz769nKioxfcySpvEBNK7ebU3uS6TuDkMZ6VyBP'\n",
    "# access_token_secret = 'bCFmnvs3QX5HmoE8oTwUHpjE2QIE8xKFRwb06pg2fksJ8'\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(contributors=None, truncated=False, text=u'Tweeting straight for Python - neat!', in_reply_to_status_id=None, id=580660224496943105, favorite_count=0, _api=<tweepy.api.API object at 0x7f0918177510>, author=User(follow_request_sent=False, profile_use_background_image=True, _json={u'follow_request_sent': False, u'profile_use_background_image': True, u'profile_text_color': u'634047', u'default_profile_image': False, u'id': 69573308, u'profile_background_image_url_https': u'https://pbs.twimg.com/profile_background_images/32686110/IMG009085425.jpg', u'verified': False, u'profile_location': None, u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/386044438/favicon2_normal.png', u'profile_sidebar_fill_color': u'E3E2DE', u'entities': {u'url': {u'urls': [{u'url': u'http://t.co/HRSJn42Niy', u'indices': [0, 22], u'expanded_url': u'http://type.hk', u'display_url': u'type.hk'}]}, u'description': {u'urls': []}}, u'followers_count': 247, u'profile_sidebar_border_color': u'D3D2CF', u'id_str': u'69573308', u'profile_background_color': u'EDECE9', u'listed_count': 15, u'is_translation_enabled': False, u'utc_offset': 28800, u'statuses_count': 236, u'description': u'a mixed message', u'friends_count': 205, u'location': u'Hong Kong, Hong Kong', u'profile_link_color': u'088253', u'profile_image_url': u'http://pbs.twimg.com/profile_images/386044438/favicon2_normal.png', u'following': False, u'geo_enabled': True, u'profile_background_image_url': u'http://pbs.twimg.com/profile_background_images/32686110/IMG009085425.jpg', u'name': u'Mart van de Ven', u'lang': u'en', u'profile_background_tile': False, u'favourites_count': 25, u'screen_name': u'tijptjik', u'notifications': False, u'url': u'http://t.co/HRSJn42Niy', u'created_at': u'Fri Aug 28 12:18:01 +0000 2009', u'contributors_enabled': False, u'time_zone': u'Hong Kong', u'protected': False, u'default_profile': False, u'is_translator': False}, time_zone=u'Hong Kong', id=69573308, _api=<tweepy.api.API object at 0x7f0918177510>, verified=False, profile_location=None, profile_image_url_https=u'https://pbs.twimg.com/profile_images/386044438/favicon2_normal.png', profile_sidebar_fill_color=u'E3E2DE', is_translator=False, geo_enabled=True, entities={u'url': {u'urls': [{u'url': u'http://t.co/HRSJn42Niy', u'indices': [0, 22], u'expanded_url': u'http://type.hk', u'display_url': u'type.hk'}]}, u'description': {u'urls': []}}, followers_count=247, protected=False, id_str=u'69573308', default_profile_image=False, listed_count=15, lang=u'en', utc_offset=28800, statuses_count=236, description=u'a mixed message', friends_count=205, profile_link_color=u'088253', profile_image_url=u'http://pbs.twimg.com/profile_images/386044438/favicon2_normal.png', notifications=False, favourites_count=25, profile_background_image_url_https=u'https://pbs.twimg.com/profile_background_images/32686110/IMG009085425.jpg', profile_background_color=u'EDECE9', profile_background_image_url=u'http://pbs.twimg.com/profile_background_images/32686110/IMG009085425.jpg', screen_name=u'tijptjik', is_translation_enabled=False, profile_background_tile=False, profile_text_color=u'634047', name=u'Mart van de Ven', url=u'http://t.co/HRSJn42Niy', created_at=datetime.datetime(2009, 8, 28, 12, 18, 1), contributors_enabled=False, location=u'Hong Kong, Hong Kong', profile_sidebar_border_color=u'D3D2CF', default_profile=False, following=False), _json={u'contributors': None, u'truncated': False, u'text': u'Tweeting straight for Python - neat!', u'in_reply_to_status_id': None, u'id': 580660224496943105, u'favorite_count': 0, u'source': u'<a href=\"https://localhost.local\" rel=\"nofollow\">Data Science 5</a>', u'retweeted': False, u'coordinates': None, u'entities': {u'symbols': [], u'user_mentions': [], u'hashtags': [], u'urls': []}, u'in_reply_to_screen_name': None, u'id_str': u'580660224496943105', u'retweet_count': 0, u'in_reply_to_user_id': None, u'favorited': False, u'user': {u'follow_request_sent': False, u'profile_use_background_image': True, u'profile_text_color': u'634047', u'default_profile_image': False, u'id': 69573308, u'profile_background_image_url_https': u'https://pbs.twimg.com/profile_background_images/32686110/IMG009085425.jpg', u'verified': False, u'profile_location': None, u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/386044438/favicon2_normal.png', u'profile_sidebar_fill_color': u'E3E2DE', u'entities': {u'url': {u'urls': [{u'url': u'http://t.co/HRSJn42Niy', u'indices': [0, 22], u'expanded_url': u'http://type.hk', u'display_url': u'type.hk'}]}, u'description': {u'urls': []}}, u'followers_count': 247, u'profile_sidebar_border_color': u'D3D2CF', u'id_str': u'69573308', u'profile_background_color': u'EDECE9', u'listed_count': 15, u'is_translation_enabled': False, u'utc_offset': 28800, u'statuses_count': 236, u'description': u'a mixed message', u'friends_count': 205, u'location': u'Hong Kong, Hong Kong', u'profile_link_color': u'088253', u'profile_image_url': u'http://pbs.twimg.com/profile_images/386044438/favicon2_normal.png', u'following': False, u'geo_enabled': True, u'profile_background_image_url': u'http://pbs.twimg.com/profile_background_images/32686110/IMG009085425.jpg', u'name': u'Mart van de Ven', u'lang': u'en', u'profile_background_tile': False, u'favourites_count': 25, u'screen_name': u'tijptjik', u'notifications': False, u'url': u'http://t.co/HRSJn42Niy', u'created_at': u'Fri Aug 28 12:18:01 +0000 2009', u'contributors_enabled': False, u'time_zone': u'Hong Kong', u'protected': False, u'default_profile': False, u'is_translator': False}, u'geo': None, u'in_reply_to_user_id_str': None, u'lang': u'en', u'created_at': u'Wed Mar 25 09:19:20 +0000 2015', u'in_reply_to_status_id_str': None, u'place': None}, coordinates=None, entities={u'symbols': [], u'user_mentions': [], u'hashtags': [], u'urls': []}, in_reply_to_screen_name=None, id_str=u'580660224496943105', retweet_count=0, in_reply_to_user_id=None, favorited=False, source_url=u'https://localhost.local', user=User(follow_request_sent=False, profile_use_background_image=True, _json={u'follow_request_sent': False, u'profile_use_background_image': True, u'profile_text_color': u'634047', u'default_profile_image': False, u'id': 69573308, u'profile_background_image_url_https': u'https://pbs.twimg.com/profile_background_images/32686110/IMG009085425.jpg', u'verified': False, u'profile_location': None, u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/386044438/favicon2_normal.png', u'profile_sidebar_fill_color': u'E3E2DE', u'entities': {u'url': {u'urls': [{u'url': u'http://t.co/HRSJn42Niy', u'indices': [0, 22], u'expanded_url': u'http://type.hk', u'display_url': u'type.hk'}]}, u'description': {u'urls': []}}, u'followers_count': 247, u'profile_sidebar_border_color': u'D3D2CF', u'id_str': u'69573308', u'profile_background_color': u'EDECE9', u'listed_count': 15, u'is_translation_enabled': False, u'utc_offset': 28800, u'statuses_count': 236, u'description': u'a mixed message', u'friends_count': 205, u'location': u'Hong Kong, Hong Kong', u'profile_link_color': u'088253', u'profile_image_url': u'http://pbs.twimg.com/profile_images/386044438/favicon2_normal.png', u'following': False, u'geo_enabled': True, u'profile_background_image_url': u'http://pbs.twimg.com/profile_background_images/32686110/IMG009085425.jpg', u'name': u'Mart van de Ven', u'lang': u'en', u'profile_background_tile': False, u'favourites_count': 25, u'screen_name': u'tijptjik', u'notifications': False, u'url': u'http://t.co/HRSJn42Niy', u'created_at': u'Fri Aug 28 12:18:01 +0000 2009', u'contributors_enabled': False, u'time_zone': u'Hong Kong', u'protected': False, u'default_profile': False, u'is_translator': False}, time_zone=u'Hong Kong', id=69573308, _api=<tweepy.api.API object at 0x7f0918177510>, verified=False, profile_location=None, profile_image_url_https=u'https://pbs.twimg.com/profile_images/386044438/favicon2_normal.png', profile_sidebar_fill_color=u'E3E2DE', is_translator=False, geo_enabled=True, entities={u'url': {u'urls': [{u'url': u'http://t.co/HRSJn42Niy', u'indices': [0, 22], u'expanded_url': u'http://type.hk', u'display_url': u'type.hk'}]}, u'description': {u'urls': []}}, followers_count=247, protected=False, id_str=u'69573308', default_profile_image=False, listed_count=15, lang=u'en', utc_offset=28800, statuses_count=236, description=u'a mixed message', friends_count=205, profile_link_color=u'088253', profile_image_url=u'http://pbs.twimg.com/profile_images/386044438/favicon2_normal.png', notifications=False, favourites_count=25, profile_background_image_url_https=u'https://pbs.twimg.com/profile_background_images/32686110/IMG009085425.jpg', profile_background_color=u'EDECE9', profile_background_image_url=u'http://pbs.twimg.com/profile_background_images/32686110/IMG009085425.jpg', screen_name=u'tijptjik', is_translation_enabled=False, profile_background_tile=False, profile_text_color=u'634047', name=u'Mart van de Ven', url=u'http://t.co/HRSJn42Niy', created_at=datetime.datetime(2009, 8, 28, 12, 18, 1), contributors_enabled=False, location=u'Hong Kong, Hong Kong', profile_sidebar_border_color=u'D3D2CF', default_profile=False, following=False), geo=None, in_reply_to_user_id_str=None, lang=u'en', created_at=datetime.datetime(2015, 3, 25, 9, 19, 20), in_reply_to_status_id_str=None, place=None, source=u'Data Science 5', retweeted=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = 'Tweeting straight for Python - neat!'\n",
    "api.update_status(status=msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom Grundy : Atlantic: Why Facebook's Offer to Host News Is so Terrifying, in One Chart http://t.co/GGRihXlfzU http://t.co/OcNgogFnDq \n",
      "\n",
      "Charles Mok 莫乃光 : 你開放咗先啦！仲叫人用微信，驚死唔俾你竊聽咩！\n",
      "北京冬奧申委稱若申辦成功　屆時互聯網會較京奧更開放\n",
      "http://t.co/I1PnhJ9DUs \n",
      "\n",
      "David Greenwood : OECD Better Life Index http://t.co/bNtQ3VkraV \n",
      "\n",
      "Dan Garrett : RT @EmeraldSoc: White &amp; Asian men dominate academic fields http://t.co/753YIaefTy Practical steps can be taken to address gender bias... \n",
      "\n",
      "Eldes Tran : Line for LKY in Singapore is insane and, no surprise, orderly https://t.co/1C2YAn85cy \n",
      "\n",
      "Beatrice Martini : RT @mollycrabapple: This morning, read @yasminelrifae  on Cairo https://t.co/Dv9F8hchRL \n",
      "\n",
      "Beatrice Martini : #civicduty RT @willowbl00: #vizthink for @katekrontiris @berkmancenter talk on #InterestedBystanders http://t.co/7zFgcIOUfO \n",
      "\n",
      "Beatrice Martini : RT @nikicheong: This is @article19org’s six part test to identify hate speech. #RightsCon http://t.co/LXNilbXfNl \n",
      "\n",
      "Beatrice Martini : RT @SamBaker: 25/3 \"I'm a big woman, I need big hair.\" Aretha Franklin, 73 today #todayimchannelling http://t.co/nnYkzJNsky \n",
      "\n",
      "clkao : RT @mySocietyPaul: mySociety's first research conference - TICTec 2015 will be starting in 10 mins time! https://t.co/776JC2HBoz #TICTeC \n",
      "\n",
      "Freakingcat : Tired after a long run on the beach and a fierce fight with the waves. Perfect day #CatBali http://t.co/OUYqSRqvjI \n",
      "\n",
      "Scott Edmunds : Things you don't want to come across while in transit: old Chinese dude in KLIA watching really noisy porn on his phone without headphones \n",
      "\n",
      "karena law : I posted a new video to Facebook http://t.co/zGhY3vnSka \n",
      "\n",
      "Open Knowledge : #OpenDataDay Recap #4, highlighting a selection of events that took place this year in Africa! http://t.co/HbLJWPJhU3 #opendata \n",
      "\n",
      "Jason Li : RT @klustout: China's Great Firewall: Fortune at the expense of freedom? http://t.co/40QUXJCi8k with @niubi @lokmantsui @amnesty's Roseann … \n",
      "\n",
      "Patrick Boehler 包蟠睿 : RT @P_Zuo: Caixin: Ex-State Security vice minister Ma Jian #马建 has 6 mistresses, 2 illegitimate kids, &amp; 6 villas http://t.co/Xg0ZvZNolg 6套别… \n",
      "\n",
      "Eldes Tran : For your daily allowance of whyonearth: Koreans have an insatiable appetite for watching strangers binge eat http://t.co/bRpefZpmhc \n",
      "\n",
      "Joanna Chiu 赵淇欣 : Will detention of women activists in China spark a \"feminist awakening\"? - China Digital Times (CDT): http://t.co/LlG5acOqKc #freethefive \n",
      "\n",
      "Bastien 偉忠 Wai-Chung : RT @CYLeung2017: Editors at @SCMP_News are so compliant. \n",
      "\n",
      "Didn't publish: news of hundreds of helpers protesting\n",
      "Did publish: a rumour one… \n",
      "\n",
      "Bastien 偉忠 Wai-Chung : RT @CYLeung2017: They're creepy and they're kooky.\n",
      "Mysterious and spooky.\n",
      "They're all together ooky.\n",
      "The CY family. http://t.co/0YweXuGdRZ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print tweet.user.name, ':', tweet.text, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for a Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lookup = 'BigData'\n",
    "\n",
    "results = []\n",
    "page = 1\n",
    "maxPage = 4\n",
    "\n",
    "while(page <= maxPage):\n",
    "    tweets = api.search(lookup, rpp=20)\n",
    "    for tweet in tweets:\n",
    "        results.append(tweet)\n",
    "    page = page + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_api',\n",
       " '_json',\n",
       " 'author',\n",
       " 'contributors',\n",
       " 'coordinates',\n",
       " 'created_at',\n",
       " 'destroy',\n",
       " 'entities',\n",
       " 'favorite',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'geo',\n",
       " 'id',\n",
       " 'id_str',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'lang',\n",
       " 'metadata',\n",
       " 'parse',\n",
       " 'parse_list',\n",
       " 'place',\n",
       " 'possibly_sensitive',\n",
       " 'retweet',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'retweeted_status',\n",
       " 'retweets',\n",
       " 'source',\n",
       " 'source_url',\n",
       " 'text',\n",
       " 'truncated',\n",
       " 'user']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge : Build up your own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Search twitter for a specific term, and build up a dataset for it.\n",
    "* Store the following columns:\n",
    "    * `created_at`\n",
    "    * `from_user`\n",
    "    * `from_user_id`\n",
    "    * `from_user_id_str`\n",
    "    * `from_user_name`\n",
    "    * `geo`\n",
    "    * `id`\n",
    "    * `iso_language_code`\n",
    "    * `source`\n",
    "    * `text`\n",
    "    * `to_user`\n",
    "    * `to_user_id`\n",
    "    * `to_user_id_str`\n",
    "    * `to_user_name`\n",
    "* What serialisation format would you use for this data?\n",
    "* Try to index by `created_at`\n",
    "* Change the index to the `datetime` type\n",
    "* Can you do some simple factor analysis based on some of the categorical features?\n",
    "* Write a word count function which counts how often  words show up in tweets about your term.\n",
    "* What are the 10 most popular terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge : Writing your dataset to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information that we really don't need. Let's keep only the data related to the tweet and insert it into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define my mongoDB database\n",
    "db = conn.twitter_results\n",
    "# Define my collection where I'll insert my search\n",
    "posts = db.posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loop through search and insert dictionary into mongoDB\n",
    "for tweet in search:\n",
    "    # Empty dictionary for storing tweet related data\n",
    "    data ={}\n",
    "    \n",
    "    # ADD YOUR DATA STRUCTURE HERE    \n",
    "    \n",
    "    # Insert process\n",
    "    posts.insert(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The _id field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MongoDB, documents stored in a collection require a unique _id field that acts as a primary key. Because ObjectIds are small, most likely unique, and fast to generate, MongoDB uses ObjectIds as the default value for the _id field if the _id field is not specified; i.e., the mongod adds the _id field and generates a unique ObjectId to assign as its value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Documents in a Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and foremost important operation we need to learn is how to retrieve our data from MongoDB. For this, Collections provide the find_one() and find() methods:\n",
    "\n",
    "The find_one() method selects and returns a single document from a collection and returns that document (or None if there are no matches). It is useful when you know there is only one matching document, or are only interested in the first match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more than a single document as the result of a query we use the find() method. find() returns a Cursor instance, which allows us to iterate over all matching documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can iterate over the first 2 documents (there are a lot in the collection and this is just an example) in the posts collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in posts.find()[:2]:\n",
    "    print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use the standard python list():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(posts.find())[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to know how many documents match a query we can perform a count() operation instead of a full query. We can get a count of all of the documents in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB queries are represented as JSON-like structure, just like documents. To build a query, you just need to specify a dictionary with the properties you wish the results to match. For example, this query will match all documents in the posts collection with ISO language code \"en\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.find({\"iso_language_code\": \"en\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to retrieve all documents with ISO language code \"en\" and source equal to \"twitterfeed\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.find({\"iso_language_code\":\"en\",\"source\":\"twitterfeed\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries can also use special query operators. These operators include gt, gte, lt, lte, ne, nin, regex, exists, not, or, and many more. The following queries show the use of some of these operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# cheat: year, month, day, hour, minute, second, microsecond\n",
    "date1 = datetime.strptime(\"18/03/15 18:30\", \"%d/%m/%y %H:%M\")\n",
    "date2 = datetime.strptime(\"18/03/13 18:05\", \"%d/%m/%y %H:%M\")\n",
    "date3 = datetime.strptime(\"18/03/15 18:10\", \"%d/%m/%y %H:%M\")\n",
    "date4 = datetime.strptime(\"18/03/15 18:25\", \"%d/%m/%y %H:%M\")\n",
    "\n",
    "cursor = posts.find({'created_at':{\"$gt\":date1}})\n",
    "cursor.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will do the same query but add the count() method to only get the count of documents that match the query. We will use count() from now onwards (easier!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.find({'created_at':{\"$gt\":date1}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tweets dates low than or equal to date2\n",
    "posts.find({'created_at':{\"$lte\":date2}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Between 2 dates\n",
    "posts.find({\"created_at\": {\"$gte\": date3, \"$lt\": date4}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Posts in either spanish or french using $or\n",
    "posts.find({\"$or\":[{\"iso_language_code\":\"es\"},{\"iso_language_code\":\"fr\"}]}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All posts except spanish or french\n",
    "posts.find({\"iso_language_code\":{\"$nin\":[\"es\",\"fr\"]}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Regex to find a post with hashtag #analytics\n",
    "import re\n",
    "regex = re.compile(r'#analytics')\n",
    "rstats = posts.find_one({\"text\":regex})\n",
    "rstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge : Reading your dataset to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Play around with queries to become comfortable with the syntax, \n",
    "* If `posts.find().sort([(\"created_at\", pymongo.ASCENDING)])` would \n",
    "give you a sorted collection based on 'created_at' field, how would you get the latest 10 tweets?\n",
    "* Write a function which retrieves all the tweets which contain words any of the top 10 most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying is all together : Blaze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Review Code](http://continuum.io/blog/blaze-expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [NOSQL DATA MODELING TECHNIQUES](https://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/)\n",
    "* [Simple CSV Data Wrangling with Python](https://districtdatalabs.silvrback.com/simple-csv-data-wrangling-with-python)\n",
    "* [A Python guide for open data file formats](http://opendata.stackexchange.com/questions/1208/a-python-guide-for-open-data-file-formats)\n",
    "* [Dive into Python 3 : XML](http://www.diveintopython3.net/xml.html)\n",
    "*  [Gentle Introduction to MongoDB using Pymongo](http://altons.github.io/python/2013/01/21/gentle-introduction-to-mongodb-using-pymongo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
